{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0f96f1-910f-438f-876f-9eff119c2b0a",
   "metadata": {
    "id": "ed0f96f1-910f-438f-876f-9eff119c2b0a"
   },
   "source": [
    "### LLM Chain 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b1e2f-9d67-4d01-9a8c-83ced6b711a9",
   "metadata": {
    "id": "5a5b1e2f-9d67-4d01-9a8c-83ced6b711a9"
   },
   "source": [
    "## 1. 환경 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b7854-f96a-47fc-b3c7-b2bdfb55df81",
   "metadata": {
    "id": "b03b7854-f96a-47fc-b3c7-b2bdfb55df81"
   },
   "source": [
    "### 1) 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
    "outputId": "c96ed02d-19b7-4e90-d92e-1ae52895e303"
   },
   "outputs": [],
   "source": [
    "# poetry add python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55152049-e9e5-4952-8e19-409f58cf3ac9",
   "metadata": {
    "id": "55152049-e9e5-4952-8e19-409f58cf3ac9"
   },
   "source": [
    "### 2) OpenAI 인증키 설정\n",
    "https://openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {
    "id": "b76f68a8-4745-4377-8057-6090b87377d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_L\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09aaca6-5aa2-4b52-bbfc-196e808dc5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.27\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01c50a-32cf-49af-891a-f9b17fa0bd6c",
   "metadata": {
    "id": "fc01c50a-32cf-49af-891a-f9b17fa0bd6c"
   },
   "source": [
    "## 2. LLM Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23729d10-9600-415b-b7d1-f954665224e3",
   "metadata": {
    "id": "23729d10-9600-415b-b7d1-f954665224e3"
   },
   "source": [
    "### 1) Prompt + LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "on0y4xF8VoyE",
   "metadata": {
    "id": "on0y4xF8VoyE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# chain 실행\n",
    "result = llm.invoke(\"인공지능 모델의 학습 원리에 대하여 쉽게 설명해 주세요.\")\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc4accf-c927-40a3-ba2c-f891c94c34f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. \\n\\n사람이 새로운 정보를 학습할 때, 뇌는 정보를 받아들이고, 처리하고, 저장하는 과정을 거칩니다. 인공지능 모델도 마찬가지로, 데이터를 받아들이고, 처리하고, 학습하는 과정을 통해 지능을 키웁니다.\\n\\n인공지능 모델의 학습 과정은 다음과 같습니다.\\n\\n1.  **데이터 수집**: 인공지능 모델은 학습을 위해 많은 데이터를 수집합니다. 이 데이터는 문제 해결에 필요한 정보입니다.\\n2.  **데이터 전처리**: 수집된 데이터는 전처리 과정을 거칩니다. 이 과정에서는 데이터를 정제하고, 변환하고, 필요한 경우 데이터를 확대하는 작업을 수행합니다.\\n3.  **모델 훈련**: 전처리된 데이터는 인공지능 모델에 입력되어 학습합니다. 모델은 입력된 데이터를 분석하고, 패턴을 찾고, 예측합니다. \\n4.  **오차 계산**: 모델의 예측 결과와 실제 결과 사이의 오차를 계산합니다. 이 오차는 모델의 성능을 평가하는 데 사용됩니다. \\n5.  **모델 업데이트**: 오차를 최소화하기 위해 모델의 매개변수를 업데이트합니다. 이 과정은 모델이 더 나은 예측을 할 수 있도록 돕습니다. \\n6.  **반복**: 모델이 만족할 만한 성능을 달성할 때까지 3\\\\~5번의 과정을 반복합니다.\\n\\n이러한 학습 과정을 통해 인공지능 모델은 데이터를 분석하고, 패턴을 찾고, 예측하는 능력을 키웁니다. 이를 통해 모델은 다양한 문제를 해결할 수 있는 지능을 갖추게 됩니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 325, 'prompt_tokens': 24, 'total_tokens': 349, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.206167666, 'prompt_time': 0.000410571, 'completion_time': 0.791493877, 'total_time': 0.791904448}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_5d3e4e58e1', 'id': 'chatcmpl-83719370-3872-4476-bcea-3486696d7a0d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--91d1355e-2636-4eb2-9f96-26ef7f5d9eb6-0' usage_metadata={'input_tokens': 24, 'output_tokens': 325, 'total_tokens': 349, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "WzcZy4PruV1n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "WzcZy4PruV1n",
    "outputId": "18ecc8f9-5748-4b16-cb07-4a0f7a01fb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. \n",
      "\n",
      "사람이 새로운 정보를 학습할 때, 뇌는 정보를 받아들이고, 처리하고, 저장하는 과정을 거칩니다. 인공지능 모델도 마찬가지로, 데이터를 받아들이고, 처리하고, 학습하는 과정을 통해 지능을 키웁니다.\n",
      "\n",
      "인공지능 모델의 학습 과정은 다음과 같습니다.\n",
      "\n",
      "1.  **데이터 수집**: 인공지능 모델은 학습을 위해 많은 데이터를 수집합니다. 이 데이터는 문제 해결에 필요한 정보입니다.\n",
      "2.  **데이터 전처리**: 수집된 데이터는 전처리 과정을 거칩니다. 이 과정에서는 데이터를 정제하고, 변환하고, 필요한 경우 데이터를 확대하는 작업을 수행합니다.\n",
      "3.  **모델 훈련**: 전처리된 데이터는 인공지능 모델에 입력되어 학습합니다. 모델은 입력된 데이터를 분석하고, 패턴을 찾고, 예측합니다. \n",
      "4.  **오차 계산**: 모델의 예측 결과와 실제 결과 사이의 오차를 계산합니다. 이 오차는 모델의 성능을 평가하는 데 사용됩니다. \n",
      "5.  **모델 업데이트**: 오차를 최소화하기 위해 모델의 매개변수를 업데이트합니다. 이 과정은 모델이 더 나은 예측을 할 수 있도록 돕습니다. \n",
      "6.  **반복**: 모델이 만족할 만한 성능을 달성할 때까지 3\\~5번의 과정을 반복합니다.\n",
      "\n",
      "이러한 학습 과정을 통해 인공지능 모델은 데이터를 분석하고, 패턴을 찾고, 예측하는 능력을 키웁니다. 이를 통해 모델은 다양한 문제를 해결할 수 있는 지능을 갖추게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c97fc",
   "metadata": {},
   "source": [
    "### 2) PromptTemplate + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "SeNi_VXqYD-b",
   "metadata": {
    "id": "SeNi_VXqYD-b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "input_variables=['input'] input_types={} partial_variables={} template='You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01WLucSpYjZt",
   "metadata": {
    "id": "01WLucSpYjZt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. 컴퓨터가 데이터를 통해 스스로 학습하고, 학습한 내용을 토대로 새로운 데이터에 대해 예측하거나 분류할 수 있도록 하는 것이죠.\\n\\n예를 들어, 고양이와 강아지의 사진을 분류하는 모델을 만든다고 가정해 봅시다.\\n\\n1. **데이터 수집**: 수많은 고양이와 강아지의 사진을 수집합니다.\\n2. **데이터 전처리**: 수집한 사진을 컴퓨터가 처리할 수 있도록 숫자로 변환합니다.\\n3. **모델 초기화**: 모델을 초기화합니다. 이 모델은 고양이와 강아지의 특징을 전혀 모르는 상태입니다.\\n4. **학습**: 모델에 고양이 사진과 강아지 사진을 보여주고, \"이것은 고양이\" 또는 \"이것은 강아지\"라고 알려줍니다. 모델은 이 정보를 바탕으로 고양이와 강아지의 특징을 스스로 학습합니다. 예를 들어, 고양이는 귀가 뾰족하고, 강아지는 귀가 쳐져있는 등입니다.\\n5. **예측**: 모델에 새로운 사진을 보여주면, 모델은 학습한 내용을 토대로 \"이것은 고양이\" 또는 \"이것은 강아지\"라고 예측합니다.\\n\\n모델은 예측과 실제 값의 차이를 최소화하도록 스스로 조정하며 학습합니다. 이 과정을 반복하면서 모델은 점점 더 정확해집니다.\\n\\n이를 수학적으로 표현하면, 다음과 같습니다.\\n\\n* **손실 함수**: 예측과 실제 값의 차이 (예: 고양이일 때 강아지라고 예측하면 손실이 크겠죠)\\n* **최적화 알고리즘**: 손실 함수를 최소화하도록 모델의 파라미터를 조정하는 알고리즘 (예: 경사 하강법)\\n\\n모델은 학습 데이터를 통해 파라미터를 조정하며, 손실 함수를 최소화하는 방향으로 학습합니다. 이를 통해 모델은 새로운 데이터에 대해 정확한 예측을 할 수 있게 됩니다.\\n\\n이렇게 인공지능 모델은 데이터를 통해 스스로 학습하고, 학습한 내용을 토대로 새로운 데이터에 대해 예측하거나 분류할 수 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 405, 'prompt_tokens': 36, 'total_tokens': 441, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.221198485, 'prompt_time': 0.000858622, 'completion_time': 0.984989228, 'total_time': 0.98584785}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_5d3e4e58e1', 'id': 'chatcmpl-71ea94c2-d68a-447a-8742-220e1a958f0d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--265c3e1c-9ae0-4ad4-88aa-222c5998663f-0' usage_metadata={'input_tokens': 36, 'output_tokens': 405, 'total_tokens': 441, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm\n",
    "print(type(chain))\n",
    "\n",
    "# chain 호출\n",
    "result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170ec878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. 컴퓨터가 데이터를 통해 스스로 학습하고, 학습한 내용을 토대로 새로운 데이터에 대해 예측하거나 분류할 수 있도록 하는 것이죠.\n",
      "\n",
      "예를 들어, 고양이와 강아지의 사진을 분류하는 모델을 만든다고 가정해 봅시다.\n",
      "\n",
      "1. **데이터 수집**: 수많은 고양이와 강아지의 사진을 수집합니다.\n",
      "2. **데이터 전처리**: 수집한 사진을 컴퓨터가 처리할 수 있도록 숫자로 변환합니다.\n",
      "3. **모델 초기화**: 모델을 초기화합니다. 이 모델은 고양이와 강아지의 특징을 전혀 모르는 상태입니다.\n",
      "4. **학습**: 모델에 고양이 사진과 강아지 사진을 보여주고, \"이것은 고양이\" 또는 \"이것은 강아지\"라고 알려줍니다. 모델은 이 정보를 바탕으로 고양이와 강아지의 특징을 스스로 학습합니다. 예를 들어, 고양이는 귀가 뾰족하고, 강아지는 귀가 쳐져있는 등입니다.\n",
      "5. **예측**: 모델에 새로운 사진을 보여주면, 모델은 학습한 내용을 토대로 \"이것은 고양이\" 또는 \"이것은 강아지\"라고 예측합니다.\n",
      "\n",
      "모델은 예측과 실제 값의 차이를 최소화하도록 스스로 조정하며 학습합니다. 이 과정을 반복하면서 모델은 점점 더 정확해집니다.\n",
      "\n",
      "이를 수학적으로 표현하면, 다음과 같습니다.\n",
      "\n",
      "* **손실 함수**: 예측과 실제 값의 차이 (예: 고양이일 때 강아지라고 예측하면 손실이 크겠죠)\n",
      "* **최적화 알고리즘**: 손실 함수를 최소화하도록 모델의 파라미터를 조정하는 알고리즘 (예: 경사 하강법)\n",
      "\n",
      "모델은 학습 데이터를 통해 파라미터를 조정하며, 손실 함수를 최소화하는 방향으로 학습합니다. 이를 통해 모델은 새로운 데이터에 대해 정확한 예측을 할 수 있게 됩니다.\n",
      "\n",
      "이렇게 인공지능 모델은 데이터를 통해 스스로 학습하고, 학습한 내용을 토대로 새로운 데이터에 대해 예측하거나 분류할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56848a4c",
   "metadata": {},
   "source": [
    "### 3) PromptTemplate + LLM(invoke()) + StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d1e9009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'str'>\n",
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. 사람의 뇌는 경험을 통해 학습하고, 이를 통해 새로운 상황에 대처할 수 있습니다. 인공지능 모델도 데이터를 통해 학습하고, 이를 통해 새로운 입력에 대한 출력을 예측할 수 있습니다.\n",
      "\n",
      "구체적으로 설명하면, 인공지능 모델의 학습 과정은 다음과 같습니다.\n",
      "\n",
      "1. **데이터 수집**: 인공지능 모델을 학습시키기 위해서는大量的한 데이터가 필요합니다. 이 데이터는 문제에 대한 입력과 출력으로 구성됩니다.\n",
      "\n",
      "2. **데이터 전처리**: 수집된 데이터는 모델에 입력되기 전에 전처리 과정을 거칩니다. 이 과정에서는 데이터의 품질을 높이고, 데이터의 크기를 줄이는 등의 작업이 수행됩니다.\n",
      "\n",
      "3. **모델 선택**: 인공지능 모델에는 여러 가지 유형이 있습니다. 문제의 성격에 따라 적합한 모델을 선택해야 합니다.\n",
      "\n",
      "4. **학습**: 선택된 모델에 전처리된 데이터를 입력하여 학습을 시작합니다. 이 과정에서 모델은 데이터의 패턴을 학습하고, 입력과 출력 사이의 관계를 발견하려고 합니다.\n",
      "\n",
      "5. **평가**: 학습이 완료된 후, 모델의 성능을 평가합니다. 이를 통해 모델의 정확도를 측정하고, 추가적인 조정이 필요한지 결정할 수 있습니다.\n",
      "\n",
      "6. **튜닝**: 모델의 성능이 만족스럽지 않은 경우, 모델의 하이퍼파рамет수를 조정하거나, 학습 데이터를 추가하는 등의 방법을 통해 모델을 개선할 수 있습니다.\n",
      "\n",
      "예를 들어, 이미지 분류 모델을 학습시키는 경우, 고양이와 강아지의 사진을大量的으로 수집하고, 이를 통해 모델이 고양이와 강아지를 구별할 수 있도록 학습시킵니다. 학습이 완료된 후, 새로운 이미지를 입력하면 모델은 이를 고양이 또는 강아지로 분류할 수 있습니다.\n",
      "\n",
      "이러한 학습 원리는 다양한 인공지능 모델에 적용될 수 있으며, 이를 통해 모델은 복잡한 문제를 해결할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. 컴포넌트 정의\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 2. chain 생성 (LCEL)\n",
    "chain = prompt | llm | output_parser\n",
    "print(type(chain))\n",
    "\n",
    "# 3. chain의 invoke 호출\n",
    "result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d997b16",
   "metadata": {},
   "source": [
    "### 4) PromptTemplate + LLM(stream()) + StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684654e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. 컴포넌트 정의\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "lm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "# 스트리밍 출력\n",
    "#print(answer)\n",
    "\n",
    "for token in answer:\n",
    "    # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0188674-915f-41af-ac46-56f9f54289b0",
   "metadata": {
    "id": "b0188674-915f-41af-ac46-56f9f54289b0"
   },
   "source": [
    "##### 2) Multiple Chains\n",
    "* Multi Chain을 활용한 영화 추천 및 줄거리 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31678c55-38a8-4ca2-b437-9d4495946b0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "31678c55-38a8-4ca2-b437-9d4495946b0a",
    "outputId": "7ee83878-5d1a-45e3-f033-100da33f3ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      " ===> 추천된 영화:\n",
      "드라마 장르라면, *문라이트 (Moonlight, 2016)*를 꼭 추천드리고 싶습니다.\n",
      "\n",
      "바리 젠킨스 감독의 이 작품은 한 흑인 소년의 성장기를 세 시기로 나누어 담아낸 영화입니다. 가족, 정체성, 사랑에 대한 이야기를 매우 섬세하고 시적인 방식으로 풀어내요. 특히 색감과 음악이 감정선을 깊게 밀어주는데, 몇몇 장면은 말이 없어도 가슴이 먹먹해질 정도예요.  \n",
      "아카데미 작품상도 받았고, 대중성보다는 울림을 중시하는 분이라면 특히 강력 추천입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추천한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 정보(제목,감독,캐스팅,줄거리)를 알려 주세요\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "movie = chain1.invoke({\"genre\": \"Drama\"})  # 영화 제목 얻기\n",
    "\n",
    "print(type(movie))\n",
    "print(\" ===> 추천된 영화:\")  # movie 값 출력\n",
    "print(movie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16718b76-f59d-48f7-906f-5d2371417803",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "16718b76-f59d-48f7-906f-5d2371417803",
    "outputId": "6e3371cd-d294-4be7-a868-2eae5ee6e5de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first={\n",
      "  movie: ChatPromptTemplate(input_variables=['genre'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['genre'], input_types={}, partial_variables={}, template='{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.'), additional_kwargs={})])\n",
      "         | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001B9429B8850>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B9429BA550>, root_client=<openai.OpenAI object at 0x000001B942981C90>, root_async_client=<openai.AsyncOpenAI object at 0x000001B9429B9190>, model_name='moonshotai/kimi-k2-instruct-0905', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
      "         | StrOutputParser()\n",
      "} middle=[ChatPromptTemplate(input_variables=['movie'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['movie'], input_types={}, partial_variables={}, template='{movie} 추천한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 정보(제목,감독,캐스팅,줄거리)를 알려 주세요'), additional_kwargs={})]), ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001B9429B8850>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B9429BA550>, root_client=<openai.OpenAI object at 0x000001B942981C90>, root_async_client=<openai.AsyncOpenAI object at 0x000001B9429B9190>, model_name='moonshotai/kimi-k2-instruct-0905', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')] last=StrOutputParser()\n",
      "\n",
      "🔹 영화 줄거리 요약:\n",
      " *그녀의 눈물이 너무 많아*  \n",
      "(원제: *Les larmes d’homme*, 2022)\n",
      "\n",
      "제목: 그녀의 눈물이 너무 많아 (Les larmes d’homme, 2022)  \n",
      "감독: 뱅자맹 루아르  \n",
      "캐스팅: 뱅자맹 루아르, 루이즈 루아르, 마르탱 루아르  \n",
      "줄거리: 남편의 갑작스러운 죽음 뒤, 장례식장에 모인 가족은 ‘슬픔’을 나누려 했지만 숨겨왔던 비밀이 하나씩 드러나면서 웃음과 눈물이 번갈아 일어난다. 하루 동안의 조용한 충돌 끝에 이들은 서로를 다시 바라보게 된다.\n"
     ]
    }
   ],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "chain2 = (\n",
    "    {\"movie\": chain1}  # chain1의 출력을 movie 변수로 전달\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(chain2)\n",
    "\n",
    "# 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "response = chain2.invoke({\"genre\": \"Drama\"})\n",
    "print(\"\\n🔹 영화 줄거리 요약:\\n\", response) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb684fd",
   "metadata": {},
   "source": [
    "##### chain1과 chain2에서 영화 제목이 일관되게 전달 되도록 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e30883ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 영화 줄거리 요약:\n",
      "('**쇼생크 탈출**\\n'\n",
      " '\\n'\n",
      " '제목: 쇼생크 탈출 (The Shawshank Redemption, 1994)  \\n'\n",
      " '감독: 프랭크 다라본트  \\n'\n",
      " '캐스팅: 팀 로빈스(앤디 듀프레인), 모건 프리먼(레드)  \\n'\n",
      " '줄거리: 은행가 앤디는 아내와 그녀의 정부를 살해한 누명을 쓰고 쇼생크 교도소에 무기징역으로 수감된다. 차분한 성격의 그는 교도소 내 '\n",
      " '악랄한 형들에게 성추행과 폭력을 당하지만, 점차 독일 교도소 관리들의 세금 조언을 도우며 특별한 지위를 얻게 된다. 20년 가까이 교도소 '\n",
      " '내에서 희망을 잃지 않고 우정을 쌓아온 앤디와 레드는 각자의 방식으로 ‘자유’를 찾아 나선다.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추천한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 정보(제목,감독,캐스팅,줄거리)를 알려 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "chain2 = (\n",
    "    {\"movie\": chain1}  # chain1의 출력을 movie 변수로 전달\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 실행: \"Drama\" 장르의 영화 추천 및 줄거리 요약\n",
    "response = chain2.invoke({\"genre\": \"Drama\"})\n",
    "print(\"\\n🔹 영화 줄거리 요약:\")\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "mylangchain-app-CGunLKE6-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
